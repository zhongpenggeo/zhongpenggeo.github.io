---
title: 梯度下降 
date: 2020-06-08 16:25:03
categories:
- DeepLearning
---




#### 随机梯度下降（SGD）

**并不是说补偿或者方向是随机的，而是指每批数据都是随机的（stochastic）**。

- 小批量SGD： 抽取训练样本x和对应目标y组成的数据批量；计算loss和梯度做一次反向传播
- 真SGD：每次去一个样本和目标
- 批量SGD：每次迭代再所有数据上。

#### SGD变体
计算下一次权重更新是还要考虑上一次权重更新；  
如：
- 带动量的SGD：动量解决了收敛速度和局部极小值点的问题。
- adagrad
- RMSprop
- 等等

